{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding catastrophic interference using Elastic Weight Consolidation (EWC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test is performed on MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable GPU in a case of kernel freezing\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrfd6FzSLrE2"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import utils\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from progressbar import ProgressBar  # pip install progressbar33\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IqHMRuH_dk83"
   },
   "outputs": [],
   "source": [
    "num_epoch = 20 # number of epochs\n",
    "num_neur = 400 # number of neurons in hidden layers\n",
    "num_class = 10 # number of classes\n",
    "img_line = 784 # input dimension\n",
    "bsize = 32 # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E5L3LUIi8PDV"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZveVc0FdPX6"
   },
   "outputs": [],
   "source": [
    "nb_samp = 60000 # total number of samples\n",
    "nb_val = 10000 # number of validation samples\n",
    "X_train = X_train.reshape(nb_samp, img_line)\n",
    "X_train = X_train.astype('float32')\n",
    "X_train /= 255\n",
    "X_test = X_test.reshape(10000, img_line)\n",
    "X_test = X_test.astype('float32')\n",
    "X_test /= 255\n",
    "\n",
    "Y_train = utils.to_categorical(y_train, 10)\n",
    "Y_test = utils.to_categorical(y_test, 10)\n",
    "\n",
    "model = Sequential()\n",
    "l2 = 1e-4 # L2 regularization\n",
    "model.add(Dense(num_neur, input_dim=img_line, activation=\"relu\",kernel_regularizer=regularizers.l2(l2)))\n",
    "model.add(Dense(num_neur, activation=\"relu\",kernel_regularizer=regularizers.l2(l2)))\n",
    "model.add(Dense(num_class, activation=\"softmax\",kernel_regularizer=regularizers.l2(l2)))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=SGD(lr=0.001), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training network  on data A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "ezoKvziJdUB4",
    "outputId": "4db84ede-5a94-4eb2-fa5f-212f25ad9165"
   },
   "outputs": [],
   "source": [
    "order_all = np.random.permutation(img_line)\n",
    "tr_a = X_train[:,order_all] # permute pixels in the images\n",
    "ts_a = X_test[:,order_all]\n",
    "\n",
    "score_a = np.zeros((num_epoch*3,2))\n",
    "history = model.fit(tr_a, Y_train, batch_size=bsize, epochs=num_epoch, validation_data=(ts_a,Y_test), verbose=0)\n",
    "score_a[:num_epoch,0] = range(num_epoch)\n",
    "score_a[:num_epoch,1] = history.history['val_acc']\n",
    "\n",
    "plt.plot(score_a[:num_epoch,0],score_a[:num_epoch,1])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Fisher information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fisher = 0 # diagonal of Hessian\n",
    "weights = model.trainable_weights # weight tensors\n",
    "input_tensors = model.inputs + model.sample_weights + model.targets + [K.learning_phase()]\n",
    "gradients = model.optimizer.get_gradients(model.total_loss, weights) # gradient tensors\n",
    "get_gradients = K.function(inputs=input_tensors, outputs=gradients)\n",
    "print('Calculating Hessian...', flush=True)\n",
    "pbar = ProgressBar(maxval=nb_val).start()\n",
    "for i in range(nb_val): # loop over all validation images\n",
    "    val_sample = tr_a[nb_samp-nb_val+i:nb_samp-nb_val+i+1,:] # get one validation image\n",
    "    y = Y_train[nb_samp-nb_val+i:nb_samp-nb_val+i+1,:] # get corresponding target\n",
    "    inputs = [val_sample, [1], y, 0]\n",
    "    grads = get_gradients(inputs)\n",
    "    Fisher += np.square(grads)\n",
    "    pbar.update(i+1)\n",
    "pbar.finish()\n",
    "Fisher = Fisher/nb_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training network on data B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save optimal weights for furher EWC training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights matrices of trained network\n",
    "w_star_l0 = model.layers[0].get_weights()[0] # learned weights of layer #0\n",
    "b_star_l0 = model.layers[0].get_weights()[1] # learned biases of layer #0\n",
    "w_star_l1 = model.layers[1].get_weights()[0] # learned weights of layer #1\n",
    "b_star_l1 = model.layers[1].get_weights()[1] # learned biases of layer #1\n",
    "w_star_l2 = model.layers[2].get_weights()[0] # learned weights of layer #2\n",
    "b_star_l2 = model.layers[2].get_weights()[1] # learned biases of layer #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define EWC regularization $\\sum_i F_i\\cdot(\\theta_i-\\theta_i^*)^2$ for every weights matrix and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 150 # importance of old task\n",
    "\n",
    "def reg_w_l0(weight_matrix): # regularization for weights in layer #0\n",
    "    return (lam/2) * K.sum(K.square(weight_matrix-w_star_l0)*Fisher[0])\n",
    "def reg_b_l0(weight_matrix): # regularization for biases in layer #0\n",
    "    return (lam/2) * K.sum(K.square(weight_matrix-b_star_l0)*Fisher[1])\n",
    "def reg_w_l1(weight_matrix): # regularization for weights in layer #1\n",
    "    return (lam/2) * K.sum(K.square(weight_matrix-w_star_l1)*Fisher[2])\n",
    "def reg_b_l1(weight_matrix): # regularization for biases in layer #1\n",
    "    return (lam/2) * K.sum(K.square(weight_matrix-b_star_l1)*Fisher[3])\n",
    "def reg_w_l2(weight_matrix): # regularization for weights in layer #2\n",
    "    return (lam/2) * K.sum(K.square(weight_matrix-w_star_l2)*Fisher[4])\n",
    "def reg_b_l2(weight_matrix): # regularization for biases in layer #2\n",
    "    return (lam/2) * K.sum(K.square(weight_matrix-b_star_l2)*Fisher[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define initializers to start from previously learned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_w_l0(shape, dtype=None): # initialize weights of layer #0\n",
    "    return w_star_l0\n",
    "def init_b_l0(shape, dtype=None): # initialize biases of layer #0\n",
    "    return b_star_l0\n",
    "def init_w_l1(shape, dtype=None): # initialize weights of layer #1\n",
    "    return w_star_l1\n",
    "def init_b_l1(shape, dtype=None): # initialize biases of layer #1\n",
    "    return b_star_l1\n",
    "def init_w_l2(shape, dtype=None): # initialize weights of layer #2\n",
    "    return w_star_l2\n",
    "def init_b_l2(shape, dtype=None): # initialize biases of layer #2\n",
    "    return b_star_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define new network for training on dataset B with EWC regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(num_neur, input_dim=img_line, activation=\"relu\",\n",
    "                kernel_initializer=init_w_l0,bias_initializer=init_b_l0,\n",
    "                kernel_regularizer=reg_w_l0,bias_regularizer=reg_b_l0))\n",
    "model.add(Dense(num_neur, activation=\"relu\",\n",
    "                kernel_initializer=init_w_l1,bias_initializer=init_b_l1,\n",
    "                kernel_regularizer=reg_w_l1,bias_regularizer=reg_b_l1))\n",
    "model.add(Dense(num_class, activation=\"softmax\",\n",
    "                kernel_initializer=init_w_l2,bias_initializer=init_b_l2,\n",
    "                kernel_regularizer=reg_w_l2,bias_regularizer=reg_b_l2))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=SGD(lr=0.001), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train network on data B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "colab_type": "code",
    "id": "pBrgbxds7cSa",
    "outputId": "b06d9ac3-83b5-41ce-c271-ee7fce0d5a00"
   },
   "outputs": [],
   "source": [
    "order_all = np.random.permutation(img_line)\n",
    "tr_b = X_train[:,order_all] # prepare permutted dataset\n",
    "ts_b = X_test[:,order_all]\n",
    "\n",
    "score_b = np.zeros((num_epoch*2,2))\n",
    "pbar = ProgressBar(maxval=num_epoch).start()\n",
    "for epoch in range(num_epoch):\n",
    "    model.fit(tr_b, Y_train, batch_size=bsize, epochs=1, verbose=0)\n",
    "    score = model.evaluate(ts_a, Y_test, verbose=0)\n",
    "    score_a[epoch+num_epoch,:] = [epoch+num_epoch,score[1]]\n",
    "    score = model.evaluate(ts_b, Y_test, verbose=0)\n",
    "    score_b[epoch,:] = [epoch+num_epoch,score[1]]\n",
    "    pbar.update(epoch+1)\n",
    "pbar.finish()\n",
    "\n",
    "plt.plot(score_a[:num_epoch*2,0],score_a[:num_epoch*2,1],label='A')\n",
    "plt.plot(score_b[:num_epoch,0],score_b[:num_epoch,1],label='B')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Fisher information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fisher = 0 # diagonal of Hessian\n",
    "weights = model.trainable_weights # weight tensors\n",
    "input_tensors = model.inputs + model.sample_weights + model.targets + [K.learning_phase()]\n",
    "gradients = model.optimizer.get_gradients(model.total_loss, weights) # gradient tensors\n",
    "get_gradients = K.function(inputs=input_tensors, outputs=gradients)\n",
    "print('Calculating Hessian...', flush=True)\n",
    "pbar = ProgressBar(maxval=2*nb_val).start()\n",
    "\n",
    "# combine validation samples from datasets A and B\n",
    "val_samples = np.vstack((tr_a[(nb_samp-nb_val):,:],tr_b[(nb_samp-nb_val):,:]))\n",
    "y_targets = np.vstack((Y_train[(nb_samp-nb_val):,:],Y_train[(nb_samp-nb_val):,:])) # get corresponding targets\n",
    "\n",
    "for i in range(2*nb_val): # loop over all validation images\n",
    "    val_sample = val_samples[i:i+1,:] # get one validation image\n",
    "    y = y_targets[i:i+1,:] # get corresponding target\n",
    "    inputs = [val_sample, [1], y, 0]\n",
    "    grads = get_gradients(inputs)\n",
    "    Fisher += np.square(grads)\n",
    "    pbar.update(i+1)\n",
    "pbar.finish()\n",
    "Fisher = Fisher/(2*nb_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training network on data C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save current optimal weights for furher EWC training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights matrices of trained network\n",
    "w_star_l0 = model.layers[0].get_weights()[0] # learned weights of layer #0\n",
    "b_star_l0 = model.layers[0].get_weights()[1] # learned biases of layer #0\n",
    "w_star_l1 = model.layers[1].get_weights()[0] # learned weights of layer #1\n",
    "b_star_l1 = model.layers[1].get_weights()[1] # learned biases of layer #1\n",
    "w_star_l2 = model.layers[2].get_weights()[0] # learned weights of layer #2\n",
    "b_star_l2 = model.layers[2].get_weights()[1] # learned biases of layer #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define new network for training on dataset C with EWC regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(num_neur, input_dim=img_line, activation=\"relu\",\n",
    "                kernel_initializer=init_w_l0,bias_initializer=init_b_l0,\n",
    "                kernel_regularizer=reg_w_l0,bias_regularizer=reg_b_l0))\n",
    "model.add(Dense(num_neur, activation=\"relu\",\n",
    "                kernel_initializer=init_w_l1,bias_initializer=init_b_l1,\n",
    "                kernel_regularizer=reg_w_l1,bias_regularizer=reg_b_l1))\n",
    "model.add(Dense(num_class, activation=\"softmax\",\n",
    "                kernel_initializer=init_w_l2,bias_initializer=init_b_l2,\n",
    "                kernel_regularizer=reg_w_l2,bias_regularizer=reg_b_l2))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=SGD(lr=0.001), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train network on dataset C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_all = np.random.permutation(img_line)\n",
    "tr_c = X_train[:,order_all]\n",
    "ts_c = X_test[:,order_all]\n",
    "\n",
    "score_c = np.zeros((num_epoch,2))\n",
    "pbar = ProgressBar(maxval=num_epoch).start()\n",
    "for epoch in range(num_epoch):\n",
    "    model.fit(tr_c, Y_train, batch_size=bsize, epochs=1, verbose=0)\n",
    "    score = model.evaluate(ts_a, Y_test, verbose=0)\n",
    "    score_a[epoch+num_epoch*2,:] = [epoch+num_epoch*2,score[1]]\n",
    "    score = model.evaluate(ts_b, Y_test, verbose=0)\n",
    "    score_b[epoch+num_epoch,:] = [epoch+num_epoch*2,score[1]]\n",
    "    score = model.evaluate(ts_c, Y_test, verbose=0)\n",
    "    score_c[epoch,:] = [epoch+num_epoch*2,score[1]]\n",
    "    pbar.update(epoch+1)\n",
    "pbar.finish()\n",
    "\n",
    "plt.plot(score_a[:,0],score_a[:,1],label='A')\n",
    "plt.plot(score_b[:,0],score_b[:,1],label='B')\n",
    "plt.plot(score_c[:,0],score_c[:,1],label='C')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training trends if necessary\n",
    "#np.savez('EWC_9',score_a=score_a,score_b=score_b,score_c=score_c)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ABC_Mnist_13.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
